{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3fa567",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/57321948/196933065-4b16c235-f3b9-4391-9cfe-4affcec87c35.png)\n",
    "\n",
    "# Submitted by: Mohammad Wasiq\n",
    "\n",
    "## Email: `gl0427@myamu.ac.in`\n",
    "\n",
    "# Pre-Placement Training Assignment - `Big Data` \n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef4477",
   "metadata": {},
   "source": [
    "**Q1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.**\n",
    "\n",
    "**Ans :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hadoop.conf')\n",
    "\n",
    "# Display the core components of Hadoop\n",
    "core_components = config['core-site']['fs.defaultFS']\n",
    "print(f\"fs.defaultFS: {core_components}\")\n",
    "\n",
    "hdfs_components = config['hdfs-site']['dfs.nameservices']\n",
    "print(f\"dfs.nameservices: {hdfs_components}\")\n",
    "\n",
    "yarn_components = config['yarn-site']['yarn.resourcemanager.hostname']\n",
    "print(f\"yarn.resourcemanager.hostname: {yarn_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4040a",
   "metadata": {},
   "source": [
    "**Q2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d33afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs\n",
    "\n",
    "def calculate_total_file_size(hdfs_host, hdfs_port, hdfs_directory):\n",
    "    # Connect to HDFS\n",
    "    hdfs = pyarrow.hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "    \n",
    "    # Get file status for each file in the directory\n",
    "    file_statuses = hdfs.ls(hdfs_directory, detail=True)\n",
    "    \n",
    "    # Calculate total file size\n",
    "    total_size = sum(fs.size for fs in file_statuses)\n",
    "    \n",
    "    # Close the HDFS connection\n",
    "    hdfs.close()\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9000\n",
    "hdfs_directory = '/user/data'\n",
    "\n",
    "total_size = calculate_total_file_size(hdfs_host, hdfs_port, hdfs_directory)\n",
    "print(f\"Total file size in HDFS directory '{hdfs_directory}': {total_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a870c",
   "metadata": {},
   "source": [
    "**Q3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2533cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--N', type=int, default=10, help='Number of top words to display')\n",
    "    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = re.findall(r'\\w+', line.lower())\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "    \n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "    \n",
    "    def reducer_find_top_words(self, _, word_count_pairs):\n",
    "        N = self.options.N\n",
    "        top_words = sorted(word_count_pairs, reverse=True)[:N]\n",
    "        for count, word in top_words:\n",
    "            yield word, count\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_words)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39242f8",
   "metadata": {},
   "source": [
    "**Q4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.**\n",
    "\n",
    "**Ans :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Hadoop cluster URL\n",
    "hadoop_url = \"http://<namenode_host>:<port>\"\n",
    "\n",
    "def check_namenode_status():\n",
    "    namenode_url = hadoop_url + \"/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    response = requests.get(namenode_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['beans'][0]['State']\n",
    "        live_nodes = data['beans'][0]['NumLiveDataNodes']\n",
    "        print(\"NameNode Status:\")\n",
    "        print(\"State: \", state)\n",
    "        print(\"Live Nodes: \", live_nodes)\n",
    "    else:\n",
    "        print(\"Error checking NameNode status.\")\n",
    "\n",
    "def check_datanode_status():\n",
    "    datanode_url = hadoop_url + \"/jmx?qry=Hadoop:service=DataNode,name=DataNodeStatus\"\n",
    "    response = requests.get(datanode_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        live_nodes = data['beans'][0]['NumLiveDataNodes']\n",
    "        dead_nodes = data['beans'][0]['NumDeadDataNodes']\n",
    "        print(\"DataNode Status:\")\n",
    "        print(\"Live Nodes: \", live_nodes)\n",
    "        print(\"Dead Nodes: \", dead_nodes)\n",
    "    else:\n",
    "        print(\"Error checking DataNode status.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_namenode_status()\n",
    "    print()\n",
    "    check_datanode_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8090",
   "metadata": {},
   "source": [
    "**Q5. Develop a Python program that lists all the files and directories in a specific HDFS path.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "def list_hdfs_path(hdfs_path):\n",
    "    fs = hdfs.connect()\n",
    "    contents = fs.ls(hdfs_path)\n",
    "    \n",
    "    print(f\"Contents of {hdfs_path}:\")\n",
    "    for item in contents:\n",
    "        print(item)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hdfs_path = '/path/to/hdfs/directory'  # Replace with your desired HDFS path\n",
    "    list_hdfs_path(hdfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b659f",
   "metadata": {},
   "source": [
    "**Q6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e58a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_data_node_storage_utilization():\n",
    "    # Retrieve DataNode information from Hadoop's REST API\n",
    "    response = requests.get('http://localhost:9870/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo')\n",
    "    data = response.json()\n",
    "    datanode_info = data['beans'][0]\n",
    "    \n",
    "    # Extract storage utilization details\n",
    "    used_storage = datanode_info['used']\n",
    "    remaining_storage = datanode_info['remaining']\n",
    "    capacity_storage = datanode_info['capacity']\n",
    "    \n",
    "    # Calculate percentage utilization\n",
    "    utilization_percentage = (used_storage / capacity_storage) * 100\n",
    "    \n",
    "    # Identify DataNodes with highest and lowest storage capacities\n",
    "    all_datanodes = datanode_info['hostName']\n",
    "    sorted_datanodes = sorted(all_datanodes, key=lambda x: capacity_storage[x])\n",
    "    highest_storage_node = sorted_datanodes[-1]\n",
    "    lowest_storage_node = sorted_datanodes[0]\n",
    "    \n",
    "    # Display storage utilization information\n",
    "    print(f\"Storage Utilization: {utilization_percentage:.2f}%\")\n",
    "    print(f\"Highest Storage Capacity Node: {highest_storage_node}\")\n",
    "    print(f\"Lowest Storage Capacity Node: {lowest_storage_node}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_data_node_storage_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa38979",
   "metadata": {},
   "source": [
    "**Q7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfe325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    # Submit Hadoop job\n",
    "    data = {\n",
    "        'jar': jar_path,\n",
    "        'class': 'org.apache.hadoop.examples.WordCount',\n",
    "        'args': [input_path, output_path]\n",
    "    }\n",
    "    response = requests.post('http://localhost:8088/ws/v1/cluster/apps/new-application')\n",
    "    app_id = response.json()['application-id']\n",
    "    requests.post(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}/application-submit', json=data)\n",
    "    \n",
    "    # Monitor job progress\n",
    "    while True:\n",
    "        response = requests.get(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}')\n",
    "        status = response.json()['app']['state']\n",
    "        if status == 'FINISHED':\n",
    "            break\n",
    "        elif status in ['FAILED', 'KILLED']:\n",
    "            print('Job execution failed.')\n",
    "            return\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # Retrieve final output\n",
    "    response = requests.get(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}/appattempts')\n",
    "    attempt_id = response.json()['appAttempts']['appAttempt'][0]['appAttemptId']\n",
    "    response = requests.get(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}/appattempts/{attempt_id}/containers')\n",
    "    container_id = response.json()['containers']['container'][0]['containerId']\n",
    "    response = requests.get(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}/appattempts/{attempt_id}/containers/{container_id}/logs')\n",
    "    logs = response.json()['containerLog']['log']\n",
    "    output = logs.split('REDUCER ')[1]\n",
    "    \n",
    "    print('Final Output:')\n",
    "    print(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    submit_hadoop_job('/path/to/hadoop-examples.jar', '/input/path', '/output/path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fbb41",
   "metadata": {},
   "source": [
    "**Q8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path, num_containers, container_memory):\n",
    "    # Submit Hadoop job\n",
    "    data = {\n",
    "        'jar': jar_path,\n",
    "        'class': 'org.apache.hadoop.examples.WordCount',\n",
    "        'args': [input_path, output_path],\n",
    "        'amContainerSpec': {\n",
    "            'commands': {\n",
    "                'command': 'yarn jar {} WordCount {} {}'.format(jar_path, input_path, output_path)\n",
    "            },\n",
    "            'resource': {\n",
    "                'vCores': 1,\n",
    "                'memory': 1024\n",
    "            }\n",
    "        },\n",
    "        'resource': {\n",
    "            'vCores': num_containers,\n",
    "            'memory': container_memory\n",
    "        }\n",
    "    }\n",
    "    response = requests.post('http://localhost:8088/ws/v1/cluster/apps/new-application')\n",
    "    app_id = response.json()['application-id']\n",
    "    requests.post(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}/application-submit', json=data)\n",
    "    \n",
    "    # Monitor job progress and resource usage\n",
    "    while True:\n",
    "        response = requests.get(f'http://localhost:8088/ws/v1/cluster/apps/{app_id}')\n",
    "        status = response.json()['app']['state']\n",
    "        if status == 'FINISHED':\n",
    "            break\n",
    "        elif status in ['FAILED', 'KILLED']:\n",
    "            print('Job execution failed.')\n",
    "            return\n",
    "        resources_used = response.json()['app']['allocatedResources']\n",
    "        print(f'Resources used: vCores - {resources_used[\"vCores\"]}, Memory - {resources_used[\"memory\"]}')\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    submit_hadoop_job('/path/to/hadoop-examples.jar', '/input/path', '/output/path', 4, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd1142",
   "metadata": {},
   "source": [
    "**Q9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.**\n",
    "\n",
    "**Ans :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class WordCountJob(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(WordCountJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', default=64, help='Input split size in MB')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word, 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    job = WordCountJob(args=['input.txt'])\n",
    "    job.run_job()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f'Execution time with default split size: {execution_time:.2f} seconds')\n",
    "\n",
    "    # Run the job with different split sizes\n",
    "    split_sizes = [64, 128, 256, 512]\n",
    "    for split_size in split_sizes:\n",
    "        start_time = time.time()\n",
    "        job = WordCountJob(args=['--split-size', str(split_size), 'input.txt'])\n",
    "        job.run_job()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f'Execution time with split size {split_size} MB: {execution_time:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
