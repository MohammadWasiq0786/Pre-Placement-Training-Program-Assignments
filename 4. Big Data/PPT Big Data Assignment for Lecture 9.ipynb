{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3fa567",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/57321948/196933065-4b16c235-f3b9-4391-9cfe-4affcec87c35.png)\n",
    "\n",
    "# Submitted by: Mohammad Wasiq\n",
    "\n",
    "## Email: `gl0427@myamu.ac.in`\n",
    "\n",
    "# Pre-Placement Training Assignment - `Big Data` \n",
    "\n",
    "## Docker, Airflow, Sqoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef4477",
   "metadata": {},
   "source": [
    "## TOPIC: Docker\n",
    "\n",
    "**Q1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e78363",
   "metadata": {},
   "outputs": [],
   "source": [
    "version: \"3\"\n",
    "services:\n",
    "  web:\n",
    "    build: ./web\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    depends_on:\n",
    "      - db\n",
    "      - cache\n",
    "  db:\n",
    "    image: postgres:latest\n",
    "    environment:\n",
    "      - POSTGRES_USER=your_username\n",
    "      - POSTGRES_PASSWORD=your_password\n",
    "      - POSTGRES_DB=your_database_name\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - ./data:/var/lib/postgresql/data\n",
    "  cache:\n",
    "    image: redis:latest\n",
    "    ports:\n",
    "      - \"6379:6379\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4040a",
   "metadata": {},
   "source": [
    "**Q2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import psutil\n",
    "\n",
    "client = docker.from_env()\n",
    "\n",
    "def get_container_cpu_usage(container_id):\n",
    "    container = client.containers.get(container_id)\n",
    "    stats = container.stats(stream=False)\n",
    "    cpu_stats = stats['cpu_stats']\n",
    "    precpu_stats = stats['precpu_stats']\n",
    "    cpu_delta = cpu_stats['cpu_usage']['total_usage'] - precpu_stats['cpu_usage']['total_usage']\n",
    "    system_delta = cpu_stats['system_cpu_usage'] - precpu_stats['system_cpu_usage']\n",
    "    cpu_percent = cpu_delta / system_delta * 100\n",
    "    return cpu_percent\n",
    "\n",
    "def scale_containers(container_name, max_replicas, cpu_threshold):\n",
    "    containers = client.containers.list(filters={'name': container_name})\n",
    "    total_cpu_usage = 0\n",
    "    for container in containers:\n",
    "        cpu_usage = get_container_cpu_usage(container.id)\n",
    "        total_cpu_usage += cpu_usage\n",
    "    average_cpu_usage = total_cpu_usage / len(containers)\n",
    "    if average_cpu_usage > cpu_threshold and len(containers) < max_replicas:\n",
    "        client.services.scale(container_name, len(containers) + 1)\n",
    "    elif average_cpu_usage < cpu_threshold and len(containers) > 1:\n",
    "        client.services.scale(container_name, len(containers) - 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    container_name = \"your_container_name\"\n",
    "    max_replicas = 5\n",
    "    cpu_threshold = 80.0\n",
    "\n",
    "    while True:\n",
    "        scale_containers(container_name, max_replicas, cpu_threshold)\n",
    "        time.sleep(60)  # Adjust the interval as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a870c",
   "metadata": {},
   "source": [
    "**Q3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_REGISTRY=\"your-registry-url\"\n",
    "DOCKER_IMAGE=\"your-image-name\"\n",
    "DOCKER_TAG=\"latest\"\n",
    "\n",
    "# Authenticate with the Docker registry\n",
    "docker login $DOCKER_REGISTRY\n",
    "\n",
    "# Pull the latest version of the image\n",
    "docker pull $DOCKER_REGISTRY/$DOCKER_IMAGE:$DOCKER_TAG\n",
    "\n",
    "# Run a container based on the latest image\n",
    "docker run -d --name my_container $DOCKER_REGISTRY/$DOCKER_IMAGE:$DOCKER_TAG\n",
    "\n",
    "# Clean up (optional)\n",
    "docker logout $DOCKER_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3c429",
   "metadata": {},
   "source": [
    "## TOPIC: Airflow\n",
    "\n",
    "**Q1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd493d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    dag_id='execute_shell_command',\n",
    "    start_date=datetime(2023, 7, 1),\n",
    "    schedule_interval='0 0 * * *'  # Runs once daily at midnight\n",
    ")\n",
    "\n",
    "# Define the task\n",
    "execute_command_task = BashOperator(\n",
    "    task_id='execute_shell_command_task',\n",
    "    bash_command='your_shell_command_here',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39242f8",
   "metadata": {},
   "source": [
    "**Q2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def process_input(input_value):\n",
    "    # Add your processing logic here\n",
    "    print(f\"Processing input: {input_value}\")\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    dag_id='dynamic_tasks',\n",
    "    start_date=datetime(2023, 7, 1),\n",
    "    schedule_interval=None  # Run manually or triggered by another DAG\n",
    ")\n",
    "\n",
    "# Define the list of inputs\n",
    "input_list = ['input1', 'input2', 'input3']\n",
    "\n",
    "# Generate tasks dynamically\n",
    "for input_value in input_list:\n",
    "    task_id = f'task_{input_value}'\n",
    "    task = PythonOperator(\n",
    "        task_id=task_id,\n",
    "        python_callable=process_input,\n",
    "        op_kwargs={'input_value': input_value},\n",
    "        dag=dag\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8090",
   "metadata": {},
   "source": [
    "**Q3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dagrun_operator import TriggerDagRunOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from datetime import datetime\n",
    "\n",
    "def check_task_a_success(**context):\n",
    "    # Check if Task A was successful\n",
    "    task_a_success = True  # Replace with your logic to check Task A success\n",
    "\n",
    "    if task_a_success:\n",
    "        return 'task_b'\n",
    "    else:\n",
    "        return 'task_c'\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    dag_id='complex_dependency',\n",
    "    start_date=datetime(2023, 7, 1),\n",
    "    schedule_interval=None  # Run manually or triggered by another DAG\n",
    ")\n",
    "\n",
    "# Define the tasks\n",
    "task_a = ...\n",
    "task_b = ...\n",
    "task_c = ...\n",
    "\n",
    "# Define the dependency\n",
    "check_task_a_success_op = BranchPythonOperator(\n",
    "    task_id='check_task_a_success',\n",
    "    python_callable=check_task_a_success,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "trigger_task_b_op = TriggerDagRunOperator(\n",
    "    task_id='trigger_task_b',\n",
    "    trigger_dag_id='complex_dependency',\n",
    "    dag=dag,\n",
    "    trigger_rule=TriggerRule.ONE_SUCCESS\n",
    ")\n",
    "\n",
    "# Set up the task dependency\n",
    "task_a >> check_task_a_success_op\n",
    "check_task_a_success_op >> [task_b, task_c]\n",
    "check_task_a_success_op >> trigger_task_b_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b659f",
   "metadata": {},
   "source": [
    "## TOPIC: Sqoop\n",
    "\n",
    "**Q1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce18a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:oracle:thin:@//host:port/service_name \\\n",
    "  --username your_username \\\n",
    "  --password-file /path/to/password_file \\\n",
    "  --table your_table \\\n",
    "  --columns \"column1,column2,column3\" \\\n",
    "  --target-dir /path/to/hadoop_directory \\\n",
    "  --as-parquetfile \\\n",
    "  --num-mappers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa38979",
   "metadata": {},
   "source": [
    "**Q2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90caf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://host:port/database \\\n",
    "  --username your_username \\\n",
    "  --password-file /path/to/password_file \\\n",
    "  --table your_table \\\n",
    "  --target-dir /path/to/hadoop_directory \\\n",
    "  --as-parquetfile \\\n",
    "  --incremental append \\\n",
    "  --check-column last_modified_date \\\n",
    "  --last-value '2022-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fbb41",
   "metadata": {},
   "source": [
    "**Q3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a807d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop export \\\n",
    "  --connect \"jdbc:sqlserver://<hostname>:<port>;database=<database>\" \\\n",
    "  --username <username> \\\n",
    "  --password-file /path/to/password_file \\\n",
    "  --table <table_name> \\\n",
    "  --export-dir /path/to/hadoop_directory \\\n",
    "  --input-fields-terminated-by ',' \\\n",
    "  --input-lines-terminated-by '\\n' \\\n",
    "  --input-null-string '\\\\N' \\\n",
    "  --input-null-non-string '\\\\N'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
