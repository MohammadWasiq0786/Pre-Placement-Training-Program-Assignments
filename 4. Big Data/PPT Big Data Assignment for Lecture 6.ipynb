{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3fa567",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/57321948/196933065-4b16c235-f3b9-4391-9cfe-4affcec87c35.png)\n",
    "\n",
    "# Submitted by: Mohammad Wasiq\n",
    "\n",
    "## Email: `gl0427@myamu.ac.in`\n",
    "\n",
    "# Pre-Placement Training Assignment - `Big Data` \n",
    "\n",
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef4477",
   "metadata": {},
   "source": [
    "**Q1. Working with RDDs:**\n",
    "\n",
    "**a) Write a Python program to create an RDD from a local data source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "result = rdd.map(lambda x: x * 2).collect()\n",
    "\n",
    "# Print the result\n",
    "print(result)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4040a",
   "metadata": {},
   "source": [
    "**b) Implement transformations and actions on the RDD to perform data processing tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a01422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformations\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
    "\n",
    "# Actions\n",
    "result = filtered_rdd.collect()\n",
    "count = filtered_rdd.count()\n",
    "sum = filtered_rdd.sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Filtered RDD: \", result)\n",
    "print(\"Count: \", count)\n",
    "print(\"Sum: \", sum)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a870c",
   "metadata": {},
   "source": [
    "**c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2418150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Operations\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Map operation to square each element\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Filter operation to select even numbers\n",
    "even_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Reduce operation to compute the sum\n",
    "sum = even_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Aggregate operation to compute sum and count\n",
    "sum_count = even_rdd.aggregate((0, 0),\n",
    "                              lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "                              lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "\n",
    "# Print the results\n",
    "print(\"Squared RDD: \", squared_rdd.collect())\n",
    "print(\"Even RDD: \", even_rdd.collect())\n",
    "print(\"Sum using Reduce: \", sum)\n",
    "print(\"Sum and Count using Aggregate: \", sum_count)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3c429",
   "metadata": {},
   "source": [
    "**Q2. Spark DataFrame Operations:**\n",
    "\n",
    "**a) Write a Python program to load a CSV file into a Spark DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5af7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Print schema and show data\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39242f8",
   "metadata": {},
   "source": [
    "**b)Perform common DataFrame operations such as filtering, grouping, or joining.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275733d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "\n",
    "# Filter rows where age is greater than 30\n",
    "filtered_df = df.filter(df.age > 30)\n",
    "\n",
    "# Filter rows where country is 'USA'\n",
    "filtered_df = df.filter(df.country == 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping And Aggregation\n",
    "\n",
    "# Group by country and calculate average age\n",
    "grouped_df = df.groupBy('country').avg('age')\n",
    "\n",
    "# Group by country and count the number of records\n",
    "grouped_df = df.groupBy('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15709b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining\n",
    "\n",
    "# Join two DataFrames based on a common column 'id'\n",
    "joined_df = df1.join(df2, on='id', how='inner')\n",
    "\n",
    "# Join two DataFrames using different join conditions\n",
    "joined_df = df1.join(df2, df1.id == df2.id, 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b8090",
   "metadata": {},
   "source": [
    "**c) Apply Spark SQL queries on the DataFrame to extract insights from the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Perform SQL queries on the DataFrame\n",
    "result = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b659f",
   "metadata": {},
   "source": [
    "**Q3. Spark Streaming:**\n",
    "\n",
    "**a) Write a Python program to create a Spark Streaming application.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that reads data from a text file stream\n",
    "lines = ssc.textFileStream(\"path/to/input/directory\")\n",
    "\n",
    "# Perform transformations on the DStream\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word counts\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa38979",
   "metadata": {},
   "source": [
    "**b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consuming data from Kafka\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"KafkaStreamingExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Define the Kafka consumer parameters\n",
    "kafkaParams = {\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"group-id\"}\n",
    "\n",
    "# Create a DStream that consumes data from Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [\"topic\"], kafkaParams)\n",
    "\n",
    "# Extract the values from the Kafka messages\n",
    "lines = kafkaStream.map(lambda x: x[1])\n",
    "\n",
    "# Perform transformations on the DStream\n",
    "# ...\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consuming data from a socket\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"SocketStreamingExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream by connecting to a socket\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Perform transformations on the DStream\n",
    "# ...\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fbb41",
   "metadata": {},
   "source": [
    "**c) Implement streaming transformations and actions to process and analyze the incoming data stream.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c46d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"StreamingWordCount\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream by connecting to a socket\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the word counts\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"StreamingFilter\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream by connecting to a socket\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Filter lines based on a condition\n",
    "filteredLines = lines.filter(lambda line: \"error\" in line)\n",
    "\n",
    "# Print the filtered lines\n",
    "filteredLines.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed operations\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "spark = SparkSession.builder.appName(\"StreamingWindow\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream by connecting to a socket\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Windowed word count\n",
    "wordCounts = lines.flatMap(lambda line: line.split(\" \")).countByValueAndWindow(windowDuration=10, slideDuration=5)\n",
    "\n",
    "# Print the word counts for each window\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd1142",
   "metadata": {},
   "source": [
    "**Q4. Spark SQL and Data Source Integration:**\n",
    "\n",
    "**a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLDatabaseIntegration\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/path/to/mysql-connector-java.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure the MySQL JDBC connection properties\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "connection_properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from a MySQL table\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties=connection_properties)\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Write data to a MySQL table\n",
    "df.write.jdbc(url=jdbc_url, table=\"newtable\", mode=\"append\", properties=connection_properties)\n",
    "\n",
    "# Close the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e050b",
   "metadata": {},
   "source": [
    "**b) Perform SQL operations on the data stored in the database using Spark SQL.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdeb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLDatabaseIntegration\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/path/to/mysql-connector-java.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure the MySQL JDBC connection properties\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "connection_properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from a MySQL table into a DataFrame\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties=connection_properties)\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "# Execute SQL queries on the table\n",
    "result = spark.sql(\"SELECT * FROM mytable WHERE column1 > 100\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Close the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a2b7c",
   "metadata": {},
   "source": [
    "**c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0455595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop Distributed File System (HDFS)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkHDFSIntegration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from HDFS into a DataFrame\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/path/to/file.csv\")\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "# ...\n",
    "\n",
    "# Write data to HDFS\n",
    "df.write.csv(\"hdfs://localhost:9000/output/path\")\n",
    "\n",
    "# Close the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon S3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkS3Integration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from Amazon S3 into a DataFrame\n",
    "df = spark.read.csv(\"s3a://bucket-name/path/to/file.csv\")\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "# ...\n",
    "\n",
    "# Write data to Amazon S3\n",
    "df.write.csv(\"s3a://bucket-name/output/path\")\n",
    "\n",
    "# Close the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
